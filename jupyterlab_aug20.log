nohup: ignoring input
[2022-07-05 13:20:32,855] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2022-07-05 13:20:32,855] [INFO] [runner.py:457:main] cmd = /home/monajati/miniconda3/envs/vlt5/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 train.py --config configs/MAGMA_v3.yml
[2022-07-05 13:20:35,371] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2022-07-05 13:20:35,371] [INFO] [launch.py:110:main] nnodes=1, num_local_procs=8, node_rank=0
[2022-07-05 13:20:35,371] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2022-07-05 13:20:35,371] [INFO] [launch.py:123:main] dist_world_size=8
[2022-07-05 13:20:35,371] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2022-07-05 13:20:39,646] [INFO] [distributed.py:49:init_distributed] Initializing torch distributed with backend: nccl
device cuda
device cudadevice
 cuda
device cuda
Loading GPTJ language model...
device cuda
device cuda
device device cuda
cuda
loading...
loading...
loading...
loading...
loading...
loading...
loading...
loading...
gpt language model is loaded
gpt language model is loaded
gpt language model is loaded
gpt language model is loaded
gpt language model is loaded
gpt language model is loaded
gpt language model is loaded
gpt language model is loaded
len_no_weight_param1 381
len_no_weight_param1 2
len_no_weight_param1 2
len_no_weight_param1 0
trainable_parameters[0].keys() dict_keys(['params', 'lr', 'weight_decay'])
len(trainable_parameters) 2
len(trainable_parameters[0]['params']) 381
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
len_no_weight_param1 381
len_no_weight_param1 2
len_no_weight_param1 2
len_no_weight_param1 0
trainable_parameters[0].keys() dict_keys(['params', 'lr', 'weight_decay'])
len(trainable_parameters) 2
len(trainable_parameters[0]['params']) 381
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
Loaded train dataset with 5000 samples
Loaded eval dataset with 5000 samples
Loaded train dataset with 5000 samples
Loaded eval1 dataset with 5000 samples
[2022-07-05 13:22:56,304] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown
len_no_weight_param1 381
len_no_weight_param1 2
len_no_weight_param1 2
len_no_weight_param1 0
trainable_parameters[0].keys() dict_keys(['params', 'lr', 'weight_decay'])
len(trainable_parameters) 2
len(trainable_parameters[0]['params']) 381
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
len_no_weight_param1 381
len_no_weight_param1 2
len_no_weight_param1 2
len_no_weight_param1 0
trainable_parameters[0].keys() dict_keys(['params', 'lr', 'weight_decay'])
len(trainable_parameters) 2
len(trainable_parameters[0]['params']) 381
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
len_no_weight_param1 381
len_no_weight_param1 2
len_no_weight_param1 2
len_no_weight_param1 0
trainable_parameters[0].keys() dict_keys(['params', 'lr', 'weight_decay'])
len(trainable_parameters) 2
len(trainable_parameters[0]['params']) 381
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
len_no_weight_param1 381
len_no_weight_param1 2
len_no_weight_param1 2
len_no_weight_param1 0
trainable_parameters[0].keys() dict_keys(['params', 'lr', 'weight_decay'])
len(trainable_parameters) 2
len(trainable_parameters[0]['params']) 381
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
len_no_weight_param1 381
len_no_weight_param1 2
len_no_weight_param1 2
len_no_weight_param1 0
trainable_parameters[0].keys() dict_keys(['params', 'lr', 'weight_decay'])
len(trainable_parameters) 2
len(trainable_parameters[0]['params']) 381
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
len_no_weight_param1 381
len_no_weight_param1 2
len_no_weight_param1 2
len_no_weight_param1 0
trainable_parameters[0].keys() dict_keys(['params', 'lr', 'weight_decay'])
len(trainable_parameters) 2
len(trainable_parameters[0]['params']) 381
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
/home/monajati/main/datasets/coco/converted_coco_val/
----------------
[2022-07-05 13:23:04,816] [INFO] [engine.py:279:__init__] DeepSpeed Flops Profiler Enabled: False
[2022-07-05 13:23:04,817] [INFO] [engine.py:1087:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer
[2022-07-05 13:23:04,817] [INFO] [engine.py:1092:_configure_optimizer] Using client Optimizer as basic optimizer
[2022-07-05 13:23:04,842] [INFO] [engine.py:1109:_configure_optimizer] DeepSpeed Basic Optimizer = Adam
[2022-07-05 13:23:04,842] [INFO] [utils.py:53:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2022-07-05 13:23:04,842] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer
[2022-07-05 13:23:04,842] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2022-07-05 13:23:04,842] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2022-07-05 13:23:04,842] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2022-07-05 13:23:04,842] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
Emitting ninja build file /home/monajati/.cache/torch_extensions/py37_cu116/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.7410314083099365 seconds
Loading extension module utils...
Time to load utils op: 0.7048764228820801 seconds
Loading extension module utils...
Time to load utils op: 0.7039477825164795 seconds
Loading extension module utils...
Time to load utils op: 0.7038261890411377 seconds
Loading extension module utils...
Time to load utils op: 0.8044593334197998 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.8030047416687012 seconds
Time to load utils op: 0.70355224609375 seconds
Loading extension module utils...
Time to load utils op: 0.8030858039855957 seconds
Rank: 2 partition count [8, 8] and sizes[(17025330, False), (787200, False)] 
Rank: 1 partition count [8, 8] and sizes[(17025330, False), (787200, False)] 
Rank: 6 partition count [8, 8] and sizes[(17025330, False), (787200, False)] 
Rank: 0 partition count [8, 8] and sizes[(17025330, False), (787200, False)] 
Rank: 3 partition count [8, 8] and sizes[(17025330, False), (787200, False)] 
Rank: 4 partition count [8, 8] and sizes[(17025330, False), (787200, False)] 
Rank: 7 partition count [8, 8] and sizes[(17025330, False), (787200, False)] 
Rank: 5 partition count [8, 8] and sizes[(17025330, False), (787200, False)] 
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003974437713623047 seconds
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0002818107604980469 seconds
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
Time to load utils op: 0.0004475116729736328 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
Time to load utils op: 0.0002810955047607422 seconds
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00029778480529785156 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004487037658691406 seconds
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005729198455810547 seconds
[2022-07-05 13:23:10,674] [INFO] [utils.py:828:see_memory_usage] Before initializing optimizer states
[2022-07-05 13:23:10,674] [INFO] [utils.py:833:see_memory_usage] MA 2.88 GB         Max_MA 5.09 GB         CA 5.55 GB         Max_CA 6 GB 
[2022-07-05 13:23:10,675] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 54.02 GB, percent = 10.7%
[2022-07-05 13:23:10,819] [INFO] [utils.py:828:see_memory_usage] After initializing optimizer states
[2022-07-05 13:23:10,820] [INFO] [utils.py:833:see_memory_usage] MA 3.01 GB         Max_MA 3.2 GB         CA 5.81 GB         Max_CA 6 GB 
[2022-07-05 13:23:10,820] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 54.16 GB, percent = 10.8%
[2022-07-05 13:23:10,820] [INFO] [stage_1_and_2.py:511:__init__] optimizer state initialized
[2022-07-05 13:23:10,983] [INFO] [utils.py:828:see_memory_usage] After initializing ZeRO optimizer
[2022-07-05 13:23:10,984] [INFO] [utils.py:833:see_memory_usage] MA 3.01 GB         Max_MA 3.01 GB         CA 5.81 GB         Max_CA 6 GB 
[2022-07-05 13:23:10,984] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 54.52 GB, percent = 10.8%
[2022-07-05 13:23:10,984] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2022-07-05 13:23:10,984] [INFO] [engine.py:786:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2022-07-05 13:23:10,984] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fe3d7de1b50>
[2022-07-05 13:23:10,984] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-07, 3e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-07-05 13:23:10,986] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   amp_enabled .................. False
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   amp_params ................... False
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   communication_data_type ...... None
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   curriculum_enabled ........... False
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   curriculum_params ............ False
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   disable_allgather ............ False
[2022-07-05 13:23:10,986] [INFO] [config.py:1063:print]   dump_state ................... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 250, 'delayed_shift': 2, 'min_scale': 1}
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   elasticity_enabled ........... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   fp16_enabled ................. True
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   global_rank .................. 0
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 8
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   loss_scale ................... 0
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   memory_breakdown ............. False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   optimizer_name ............... None
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   optimizer_params ............. None
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   pld_enabled .................. False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   pld_params ................... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   prescale_gradients ........... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   quantize_groups .............. 1
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   quantize_offset .............. 1000
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   quantize_period .............. 1000
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   quantize_rounding ............ 0
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   quantize_training_enabled .... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   quantize_type ................ 0
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   quantize_verbose ............. False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   scheduler_name ............... WarmupDecayLR
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   scheduler_params ............. {'total_num_steps': 300000, 'warmup_min_lr': [0.0, 0.0], 'warmup_max_lr': [2e-07, 3e-06], 'warmup_num_steps': 100}
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   sparse_attention ............. None
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   steps_per_print .............. 10
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   tensorboard_output_path ...... 
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   train_batch_size ............. 64
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  1
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   world_size ................... 8
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  True
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   zero_config .................. {
    "stage": 2, 
    "contiguous_gradients": true, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 5.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 5.000000e+08, 
    "overlap_comm": false, 
    "load_from_fp32_weights": false, 
    "elastic_checkpoint": false, 
    "offload_param": null, 
    "offload_optimizer": null, 
    "sub_group_size": 1.000000e+09, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_16bit_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "round_robin_gradients": false, 
    "legacy_stage1": false
}
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   zero_enabled ................. True
[2022-07-05 13:23:10,987] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 2
[2022-07-05 13:23:10,988] [INFO] [config.py:1071:print]   json = {
    "train_batch_size": 64, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "fp16": {
        "enabled": true, 
        "loss_scale_window": 250
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "total_num_steps": 3.000000e+05, 
            "warmup_min_lr": [0.0, 0.0], 
            "warmup_max_lr": [2e-07, 3e-06], 
            "warmup_num_steps": 100
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "load_from_fp32_weights": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "train_micro_batch_size_per_gpu": 1
}
Using /home/monajati/.cache/torch_extensions/py37_cu116 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00030303001403808594 seconds
training...:   0%|                                                                                                       | 0/150000 [00:00<?, ?it/s]wandb: Currently logged in as: monajati. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.21 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.20
wandb: Run data is saved locally in /home/monajati/main/metaVL/magma/wandb/run-20220705_132311-11yfmu94
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run c64022b8
wandb: â­ï¸ View project at https://wandb.ai/monajati/magma
wandb: ðŸš€ View run at https://wandb.ai/monajati/magma/runs/11yfmu94
[2022-07-05 13:23:23,060] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296
[2022-07-05 13:23:23,063] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:23:23,063] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
current_lr [0.0]
[2022-07-05 13:23:23,064] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:23:23,064] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]current_lr
 [0.0]
[2022-07-05 13:23:23,064] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:23,064] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 1 Loss: 15.671875:   0%|                                                                               | 0/150000 [00:12<?, ?it/s]training... Step: 1 Loss: 15.671875:   0%|                                                                   | 1/150000 [00:12<503:14:11, 12.08s/it][2022-07-05 13:23:23,070] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:23,066] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:24,120] [INFO] [timer.py:201:stop] 0/10, SamplesPerSec=15.467508578032263, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:23:27,206] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0
[2022-07-05 13:23:27,209] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:27,209] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:27,209] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr[2022-07-05 13:23:27,209] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
 [2022-07-05 13:23:27,209] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[0.0]current_lr
 current_lr[0.0] 
[0.0]
[2022-07-05 13:23:27,209] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 2 Loss: 15.6875:   0%|                                                                     | 1/150000 [00:16<503:14:11, 12.08s/it][2022-07-05 13:23:27,223] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr[2022-07-05 13:23:27,227] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
 [0.0]
training... Step: 2 Loss: 15.6875:   0%|                                                                     | 2/150000 [00:16<309:17:05,  7.42s/it][2022-07-05 13:23:29,284] [INFO] [timer.py:201:stop] 0/20, SamplesPerSec=15.53142887295704, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:23:31,340] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0
[2022-07-05 13:23:31,343] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:31,343] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:31,343] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:31,343] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 3 Loss: 15.4765625:   0%|                                                                  | 2/150000 [00:20<309:17:05,  7.42s/it][2022-07-05 13:23:31,343] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
training... Step: 3 Loss: 15.4765625:   0%|                                                                  | 3/150000 [00:20<246:18:35,  5.91s/it][2022-07-05 13:23:31,344] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
current_lr [0.0]
[2022-07-05 13:23:31,350] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:31,351] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:34,630] [INFO] [timer.py:201:stop] 0/30, SamplesPerSec=15.329895891546421, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:23:35,653] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0
[2022-07-05 13:23:35,656] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 4 Loss: 15.5859375:   0%|                                                                  | 3/150000 [00:24<246:18:35,  5.91s/it][2022-07-05 13:23:35,656] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [2022-07-05 13:23:35,656] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[0.0]
training... Step: 4 Loss: 15.5859375:   0%|                                                                  | 4/150000 [00:24<220:00:07,  5.28s/it][2022-07-05 13:23:35,656] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:35,656] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:35,656] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:35,657] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:23:35,657] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]current_lr
 [0.0]
[2022-07-05 13:23:39,767] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0
[2022-07-05 13:23:39,768] [INFO] [timer.py:201:stop] 0/40, SamplesPerSec=15.40373525731731, MemAllocated=3.2GB, MaxMemAllocated=5.91GB
[2022-07-05 13:23:39,771] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:39,771] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:39,771] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:39,771] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:39,771] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:39,771] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:39,777] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 5 Loss: 15.703125:   0%|                                                                   | 4/150000 [00:28<220:00:07,  5.28s/it][2022-07-05 13:23:39,778] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 5 Loss: 15.703125:   0%|                                                                   | 5/150000 [00:28<202:52:22,  4.87s/it][2022-07-05 13:23:43,896] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0
[2022-07-05 13:23:43,899] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:43,898] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0][2022-07-05 13:23:43,899] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started

current_lr [0.0]
[2022-07-05 13:23:43,899] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:43,900] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:43,900] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 6 Loss: 15.4140625:   0%|                                                                  | 5/150000 [00:32<202:52:22,  4.87s/it][2022-07-05 13:23:43,906] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:43,917] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 6 Loss: 15.4140625:   0%|                                                                  | 6/150000 [00:32<192:12:05,  4.61s/it][2022-07-05 13:23:44,960] [INFO] [timer.py:201:stop] 0/50, SamplesPerSec=15.442530032875473, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:23:48,051] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0
[2022-07-05 13:23:48,053] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:48,054] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:48,054] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:23:48,054] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]current_lr
 [0.0]
[2022-07-05 13:23:48,054] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:48,054] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 7 Loss: 15.8359375:   0%|                                                                  | 6/150000 [00:37<192:12:05,  4.61s/it][2022-07-05 13:23:48,066] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [2022-07-05 13:23:48,066] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[0.0]
training... Step: 7 Loss: 15.8359375:   0%|                                                                  | 7/150000 [00:37<186:03:00,  4.47s/it][2022-07-05 13:23:50,123] [INFO] [timer.py:201:stop] 0/60, SamplesPerSec=15.468716350457758, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:23:52,170] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0
[2022-07-05 13:23:52,173] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:52,173] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:52,173] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
training... Step: 8 Loss: 15.4921875:   0%|                                                                  | 7/150000 [00:41<186:03:00,  4.47s/it]current_lr [0.0]
[2022-07-05 13:23:52,173] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:23:52,173] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:52,174] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
current_lr [0.0]
training... Step: 8 Loss: 15.4921875:   0%|                                                                  | 8/150000 [00:41<181:12:00,  4.35s/it][2022-07-05 13:23:52,180] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:52,191] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:55,286] [INFO] [timer.py:201:stop] 0/70, SamplesPerSec=15.479414070779583, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:23:56,309] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0
[2022-07-05 13:23:56,312] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr[2022-07-05 13:23:56,312] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
 [0.0]
current_lr [0.0]
training... Step: 9 Loss: 15.5625:   0%|                                                                     | 8/150000 [00:45<181:12:00,  4.35s/it][2022-07-05 13:23:56,313] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr[2022-07-05 13:23:56,313] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:56,314] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:56,314] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
 [0.0][2022-07-05 13:23:56,312] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:23:56,320] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]

training... Step: 9 Loss: 15.5625:   0%|                                                                     | 9/150000 [00:45<178:38:44,  4.29s/it][2022-07-05 13:24:00,449] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0
[2022-07-05 13:24:00,449] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=10, lr=[2e-07, 3e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-07-05 13:24:00,450] [INFO] [timer.py:201:stop] 0/80, SamplesPerSec=15.489169435310323, MemAllocated=3.2GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:00,452] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:00,452] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:00,453] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:00,453] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:00,453] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:00,453] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 10 Loss: 15.421875:   0%|                                                                  | 9/150000 [00:49<178:38:44,  4.29s/it][2022-07-05 13:24:00,465] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr[2022-07-05 13:24:00,466] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
 [0.0]
training... Step: 10 Loss: 15.421875:   0%|                                                                 | 10/150000 [00:49<176:49:49,  4.24s/it][2022-07-05 13:24:04,607] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0
[2022-07-05 13:24:04,609] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:04,609] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:04,610] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:24:04,610] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
current_lr [0.0]
training... Step: 11 Loss: 15.3359375:   0%|                                                                | 10/150000 [00:53<176:49:49,  4.24s/it]training... Step: 11 Loss: 15.3359375:   0%|                                                                | 11/150000 [00:53<175:28:00,  4.21s/it][2022-07-05 13:24:04,611] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:04,611] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:04,627] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:04,641] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:05,667] [INFO] [timer.py:201:stop] 0/90, SamplesPerSec=15.485156209873326, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:08,768] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0
[2022-07-05 13:24:08,770] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:08,771] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:08,771] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:08,772] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:08,772] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:08,771] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 12 Loss: 15.578125:   0%|                                                                 | 11/150000 [00:57<175:28:00,  4.21s/it]training... Step: 12 Loss: 15.578125:   0%|                                                                 | 12/150000 [00:57<174:53:34,  4.20s/it][2022-07-05 13:24:08,778] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:08,790] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:10,936] [INFO] [timer.py:201:stop] 0/100, SamplesPerSec=15.456900783818337, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:13,004] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0
training... Step: 13 Loss: 15.640625:   0%|                                                                 | 12/150000 [01:02<174:53:34,  4.20s/it][2022-07-05 13:24:13,006] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:13,007] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:13,007] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
training... Step: 13 Loss: 15.640625:   0%|                                                                 | 13/150000 [01:02<175:17:36,  4.21s/it]current_lr [0.0]
[2022-07-05 13:24:13,007] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:13,007] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:13,007] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:13,007] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:13,016] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:16,131] [INFO] [timer.py:201:stop] 0/110, SamplesPerSec=15.452535780789946, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:17,155] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
[2022-07-05 13:24:17,158] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:17,158] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:17,159] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:17,159] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:17,160] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:17,165] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 14 Loss: 15.7890625:   0%|                                                                | 13/150000 [01:06<175:17:36,  4.21s/it][2022-07-05 13:24:17,166] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:17,159] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 14 Loss: 15.7890625:   0%|                                                                | 14/150000 [01:06<174:44:08,  4.19s/it][2022-07-05 13:24:21,292] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
[2022-07-05 13:24:21,293] [INFO] [timer.py:201:stop] 0/120, SamplesPerSec=15.460458147561646, MemAllocated=3.2GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:21,295] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:21,295] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:21,295] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:21,295] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:21,295] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 15 Loss: 15.609375:   0%|                                                                 | 14/150000 [01:10<174:44:08,  4.19s/it][2022-07-05 13:24:21,296] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 15 Loss: 15.609375:   0%|                                                                 | 15/150000 [01:10<173:56:53,  4.18s/it][2022-07-05 13:24:21,303] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:21,309] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:25,447] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2022-07-05 13:24:25,450] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:25,450] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:25,450] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:25,450] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:24:25,450] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lrcurrent_lr  [0.0]
[0.0]
[2022-07-05 13:24:25,450] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:25,451] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr[2022-07-05 13:24:25,450] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 16 Loss: 15.7734375:   0%|                                                                | 15/150000 [01:14<173:56:53,  4.18s/it] [0.0]
training... Step: 16 Loss: 15.7734375:   0%|                                                                | 16/150000 [01:14<173:46:01,  4.17s/it][2022-07-05 13:24:26,488] [INFO] [timer.py:201:stop] 0/130, SamplesPerSec=15.46164702591749, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:29,586] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2022-07-05 13:24:29,589] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 17 Loss: 15.6015625:   0%|                                                                | 16/150000 [01:18<173:46:01,  4.17s/it][2022-07-05 13:24:29,589] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:29,590] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 17 Loss: 15.6015625:   0%|                                                                | 17/150000 [01:18<173:13:21,  4.16s/it][2022-07-05 13:24:29,590] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:29,591] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:29,590] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:29,590] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:29,596] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:31,659] [INFO] [timer.py:201:stop] 0/140, SamplesPerSec=15.463349174301804, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:33,707] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2022-07-05 13:24:33,709] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:33,709] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:33,710] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:33,710] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:33,711] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 18 Loss: 15.421875:   0%|                                                                 | 17/150000 [01:22<173:13:21,  4.16s/it][2022-07-05 13:24:33,710] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:33,717] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:33,710] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 18 Loss: 15.421875:   0%|                                                                 | 18/150000 [01:22<172:53:30,  4.15s/it][2022-07-05 13:24:36,831] [INFO] [timer.py:201:stop] 0/150, SamplesPerSec=15.469307720609942, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:37,849] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2022-07-05 13:24:37,852] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 19 Loss: 15.4140625:   0%|                                                                | 18/150000 [01:26<172:53:30,  4.15s/it][2022-07-05 13:24:37,852] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:37,852] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:37,852] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:37,853] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:37,853] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 19 Loss: 15.4140625:   0%|                                                                | 19/150000 [01:26<172:39:24,  4.14s/it][2022-07-05 13:24:37,853] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:37,859] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:42,117] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2022-07-05 13:24:42,117] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=20, lr=[2e-07, 3e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-07-05 13:24:42,117] [INFO] [timer.py:201:stop] 0/160, SamplesPerSec=15.448314851210947, MemAllocated=3.2GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:42,120] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 20 Loss: 15.578125:   0%|                                                                 | 19/150000 [01:31<172:39:24,  4.14s/it][2022-07-05 13:24:42,119] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:42,120] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:42,120] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:42,120] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:42,120] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:42,121] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:42,122] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 20 Loss: 15.578125:   0%|                                                                 | 20/150000 [01:31<174:17:29,  4.18s/it][2022-07-05 13:24:46,274] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2022-07-05 13:24:46,277] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 21 Loss: 15.5625:   0%|                                                                   | 20/150000 [01:35<174:17:29,  4.18s/it][2022-07-05 13:24:46,277] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:46,277] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr[2022-07-05 13:24:46,277] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
 [0.0]
current_lr [0.0]
[2022-07-05 13:24:46,278] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:46,278] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:46,278] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:46,278] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 21 Loss: 15.5625:   0%|                                                                   | 21/150000 [01:35<173:52:25,  4.17s/it][2022-07-05 13:24:47,327] [INFO] [timer.py:201:stop] 0/170, SamplesPerSec=15.446154570880523, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:50,446] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0
[2022-07-05 13:24:50,449] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:50,449] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0][2022-07-05 13:24:50,449] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started

current_lr [0.0]
[2022-07-05 13:24:50,450] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:50,451] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:50,451] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 22 Loss: 15.8359375:   0%|                                                                | 21/150000 [01:39<173:52:25,  4.17s/it][2022-07-05 13:24:50,451] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:50,451] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 22 Loss: 15.8359375:   0%|                                                                | 22/150000 [01:39<173:52:11,  4.17s/it][2022-07-05 13:24:52,548] [INFO] [timer.py:201:stop] 0/180, SamplesPerSec=15.441901390730571, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:54,610] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
[2022-07-05 13:24:54,612] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:54,613] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:54,613] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
training... Step: 23 Loss: 15.6171875:   0%|                                                                | 22/150000 [01:43<173:52:11,  4.17s/it]current_lr [0.0]
[2022-07-05 13:24:54,613] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:24:54,613] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:54,613] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:54,613] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:24:54,613] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
current_lr [0.0]
current_lr [0.0]
training... Step: 23 Loss: 15.6171875:   0%|                                                                | 23/150000 [01:43<173:52:19,  4.17s/it][2022-07-05 13:24:57,743] [INFO] [timer.py:201:stop] 0/190, SamplesPerSec=15.44282041141947, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:24:58,789] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024.0, reducing to 512.0
[2022-07-05 13:24:58,792] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:58,792] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 24 Loss: 15.515625:   0%|                                                                 | 23/150000 [01:47<173:52:19,  4.17s/it][2022-07-05 13:24:58,792] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:58,793] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:58,793] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 24 Loss: 15.515625:   0%|                                                                 | 24/150000 [01:47<173:48:07,  4.17s/it][2022-07-05 13:24:58,793] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:58,793] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:24:58,811] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:02,954] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512.0, reducing to 256.0
[2022-07-05 13:25:02,955] [INFO] [timer.py:201:stop] 0/200, SamplesPerSec=15.439997651108222, MemAllocated=3.2GB, MaxMemAllocated=5.91GB
[2022-07-05 13:25:02,957] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:02,958] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:02,957] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:02,958] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr[2022-07-05 13:25:02,958] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:02,958] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 25 Loss: 15.53125:   0%|                                                                  | 24/150000 [01:51<173:48:07,  4.17s/it][2022-07-05 13:25:02,958] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:02,959] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
 [0.0]
training... Step: 25 Loss: 15.53125:   0%|                                                                  | 25/150000 [01:51<173:42:56,  4.17s/it][2022-07-05 13:25:07,180] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256.0, reducing to 128.0
[2022-07-05 13:25:07,182] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:07,183] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:07,183] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:07,183] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:07,184] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:07,183] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:07,185] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 26 Loss: 15.4296875:   0%|                                                                | 25/150000 [01:56<173:42:56,  4.17s/it]training... Step: 26 Loss: 15.4296875:   0%|                                                                | 26/150000 [01:56<174:24:11,  4.19s/it][2022-07-05 13:25:07,190] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:08,211] [INFO] [timer.py:201:stop] 0/210, SamplesPerSec=15.432309720800756, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:25:11,318] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128.0, reducing to 64.0
[2022-07-05 13:25:11,321] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:11,321] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 27 Loss: 15.609375:   0%|                                                                 | 26/150000 [02:00<174:24:11,  4.19s/it][2022-07-05 13:25:11,322] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:11,321] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:11,322] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:11,323] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:11,329] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 27 Loss: 15.609375:   0%|                                                                 | 27/150000 [02:00<173:48:36,  4.17s/it][2022-07-05 13:25:11,341] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:13,420] [INFO] [timer.py:201:stop] 0/220, SamplesPerSec=15.429890164847313, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:25:15,474] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64.0, reducing to 32.0
[2022-07-05 13:25:15,476] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr[2022-07-05 13:25:15,476] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
 [0.0]
current_lr [0.0]
[2022-07-05 13:25:15,476] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:15,476] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
training... Step: 28 Loss: 15.6796875:   0%|                                                                | 27/150000 [02:04<173:48:36,  4.17s/it]current_lr [0.0]
[2022-07-05 13:25:15,476] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:15,477] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:15,477] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:15,489] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 28 Loss: 15.6796875:   0%|                                                                | 28/150000 [02:04<173:35:19,  4.17s/it][2022-07-05 13:25:18,596] [INFO] [timer.py:201:stop] 0/230, SamplesPerSec=15.431540242279892, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:25:19,630] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32.0, reducing to 16.0
[2022-07-05 13:25:19,632] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:25:19,633] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
current_lr [0.0]
[2022-07-05 13:25:19,633] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:19,634] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:19,634] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:19,634] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:19,634] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:19,634] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 29 Loss: 15.734375:   0%|                                                                 | 28/150000 [02:08<173:35:19,  4.17s/it]training... Step: 29 Loss: 15.734375:   0%|                                                                 | 29/150000 [02:08<173:28:37,  4.16s/it][2022-07-05 13:25:23,769] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16.0, reducing to 8.0
[2022-07-05 13:25:23,769] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=30, lr=[2e-07, 3e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-07-05 13:25:23,770] [INFO] [timer.py:201:stop] 0/240, SamplesPerSec=15.434214146460727, MemAllocated=3.2GB, MaxMemAllocated=5.91GB
[2022-07-05 13:25:23,772] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:25:23,772] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]current_lr
 [0.0]
[2022-07-05 13:25:23,772] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:23,773] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:23,772] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:23,773] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:23,779] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 30 Loss: 15.515625:   0%|                                                                 | 29/150000 [02:12<173:28:37,  4.16s/it]training... Step: 30 Loss: 15.515625:   0%|                                                                 | 30/150000 [02:12<173:08:43,  4.16s/it][2022-07-05 13:25:23,785] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:27,919] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8.0, reducing to 4.0
[2022-07-05 13:25:27,922] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:27,922] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:27,922] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:27,922] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:27,923] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:27,923] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:27,929] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 31 Loss: 15.046875:   0%|                                                                 | 30/150000 [02:16<173:08:43,  4.16s/it]training... Step: 31 Loss: 15.046875:   0%|                                                                 | 31/150000 [02:16<173:08:02,  4.16s/it][2022-07-05 13:25:27,940] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:28,951] [INFO] [timer.py:201:stop] 0/250, SamplesPerSec=15.435868556028057, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:25:32,048] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4.0, reducing to 2.0
[2022-07-05 13:25:32,051] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:32,051] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:32,051] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:25:32,051] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:25:32,051] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lrcurrent_lr  [0.0][0.0]current_lr
 
[0.0]
[2022-07-05 13:25:32,051] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:32,052] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 32 Loss: 15.484375:   0%|                                                                 | 31/150000 [02:21<173:08:02,  4.16s/it][2022-07-05 13:25:32,060] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 32 Loss: 15.484375:   0%|                                                                 | 32/150000 [02:21<172:50:09,  4.15s/it][2022-07-05 13:25:34,126] [INFO] [timer.py:201:stop] 0/260, SamplesPerSec=15.438211121593657, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
[2022-07-05 13:25:36,198] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2.0, reducing to 1.0
[2022-07-05 13:25:36,201] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:36,201] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:36,201] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:36,202] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:36,202] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
training... Step: 33 Loss: 15.4765625:   0%|                                                                | 32/150000 [02:25<172:50:09,  4.15s/it][2022-07-05 13:25:36,213] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
[2022-07-05 13:25:36,220] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
current_lr [0.0]
training... Step: 33 Loss: 15.4765625:   0%|                                                                | 33/150000 [02:25<172:59:46,  4.15s/it][2022-07-05 13:25:36,227] [WARNING] [lr_schedules.py:754:get_lr] Attempting to get learning rate from scheduler before it has started
current_lr [0.0]
[2022-07-05 13:25:39,331] [INFO] [timer.py:201:stop] 0/270, SamplesPerSec=15.43855144554549, MemAllocated=3.25GB, MaxMemAllocated=5.91GB
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "train.py", line 203, in <module>
Traceback (most recent call last):
  File "train.py", line 203, in <module>
  File "train.py", line 203, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "train.py", line 203, in <module>
  File "train.py", line 203, in <module>
  File "train.py", line 203, in <module>
  File "train.py", line 203, in <module>
    loss = train_step(config, train_loader, model_engine)
  File "/home/monajati/main/metaVL/magma/magma/train_loop.py", line 19, in train_step
        loss = train_step(config, train_loader, model_engine)model_engine.step()

      File "/home/monajati/main/metaVL/magma/magma/train_loop.py", line 19, in train_step
loss = train_step(config, train_loader, model_engine)  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1930, in step

  File "/home/monajati/main/metaVL/magma/magma/train_loop.py", line 19, in train_step
    model_engine.step()    
model_engine.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1930, in step
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1930, in step
training... Step: 33 Loss: 15.4765625:   0%|                                                                | 33/150000 [02:29<188:33:54,  4.53s/it]
    loss = train_step(config, train_loader, model_engine)
  File "/home/monajati/main/metaVL/magma/magma/train_loop.py", line 19, in train_step
    loss = train_step(config, train_loader, model_engine)
  File "/home/monajati/main/metaVL/magma/magma/train_loop.py", line 19, in train_step
    model_engine.step()
      File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1930, in step
model_engine.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1930, in step
    loss = train_step(config, train_loader, model_engine)
  File "/home/monajati/main/metaVL/magma/magma/train_loop.py", line 19, in train_step
    model_engine.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1930, in step
    self._take_model_step(lr_kwargs)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1831, in _take_model_step
    loss = train_step(config, train_loader, model_engine)
  File "/home/monajati/main/metaVL/magma/magma/train_loop.py", line 19, in train_step
        self._take_model_step(lr_kwargs)    self._take_model_step(lr_kwargs)

model_engine.step()  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1831, in _take_model_step
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1831, in _take_model_step

  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1930, in step
    self._take_model_step(lr_kwargs)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1831, in _take_model_step
    self._take_model_step(lr_kwargs)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1831, in _take_model_step
    self.optimizer.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1667, in step
    self._take_model_step(lr_kwargs)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1831, in _take_model_step
    self.optimizer.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1667, in step
    self.optimizer.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1667, in step
    self._take_model_step(lr_kwargs)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1831, in _take_model_step
    self.optimizer.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1667, in step
    self.optimizer.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1667, in step
    self._update_scale(self.overflow)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1922, in _update_scale
    self.optimizer.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1667, in step
        self._update_scale(self.overflow)self._update_scale(self.overflow)

  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1922, in _update_scale
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1922, in _update_scale
    self.optimizer.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1667, in step
    self._update_scale(self.overflow)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1922, in _update_scale
    self._update_scale(self.overflow)
Traceback (most recent call last):
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1922, in _update_scale
  File "train.py", line 203, in <module>
    loss = train_step(config, train_loader, model_engine)
  File "/home/monajati/main/metaVL/magma/magma/train_loop.py", line 19, in train_step
    model_engine.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1930, in step
    self._take_model_step(lr_kwargs)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/engine.py", line 1831, in _take_model_step
    self.optimizer.step()
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1667, in step
    self._update_scale(self.overflow)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1922, in _update_scale
    self.loss_scaler.update_scale(has_overflow)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 157, in update_scale
    "Current loss scale already at minimum - cannot decrease scale anymore. Exiting run."
        self._update_scale(self.overflow)self.loss_scaler.update_scale(has_overflow)

  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1922, in _update_scale
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 157, in update_scale
            "Current loss scale already at minimum - cannot decrease scale anymore. Exiting run."self.loss_scaler.update_scale(has_overflow)self.loss_scaler.update_scale(has_overflow)

    
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 157, in update_scale
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 157, in update_scale
self._update_scale(self.overflow)Exception
: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1922, in _update_scale

        "Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.""Current loss scale already at minimum - cannot decrease scale anymore. Exiting run."

Exception: ExceptionCurrent loss scale already at minimum - cannot decrease scale anymore. Exiting run.: 
Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
    self.loss_scaler.update_scale(has_overflow)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 157, in update_scale
    self.loss_scaler.update_scale(has_overflow)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 157, in update_scale
Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
    "Current loss scale already at minimum - cannot decrease scale anymore. Exiting run."
Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
    "Current loss scale already at minimum - cannot decrease scale anymore. Exiting run."
Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
    self.loss_scaler.update_scale(has_overflow)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 157, in update_scale
    "Current loss scale already at minimum - cannot decrease scale anymore. Exiting run."
Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
    self.loss_scaler.update_scale(has_overflow)
  File "/home/monajati/miniconda3/envs/vlt5/lib/python3.7/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 157, in update_scale
    "Current loss scale already at minimum - cannot decrease scale anymore. Exiting run."
Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: \ 0.029 MB of 0.029 MB uploaded (0.000 MB deduped)[2022-07-05 13:25:44,749] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 316072
[2022-07-05 13:25:44,750] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 316073
[2022-07-05 13:25:44,750] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 316074
[2022-07-05 13:25:44,750] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 316075
[2022-07-05 13:25:44,750] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 316076
[2022-07-05 13:25:44,750] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 316077
[2022-07-05 13:25:44,750] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 316080
[2022-07-05 13:25:44,750] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 316083
[2022-07-05 13:25:44,750] [ERROR] [launch.py:184:sigkill_handler] ['/home/monajati/miniconda3/envs/vlt5/bin/python', '-u', 'train.py', '--local_rank=7', '--config', 'configs/MAGMA_v3.yml'] exits with return code = 1
/home/monajati/miniconda3/envs/vlt5/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown
  len(cache))
